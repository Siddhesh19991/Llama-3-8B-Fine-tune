# Llama-3-8B-Fine-tune

This repository demonstrates the fine-tuning of the Llama-3-8B model and compares its performance with the base model. It is part of a comparative study between fine-tuning and RAG (Retrieval-Augmented Generation) to determine which approach is more suitable for our use case.

The detailed blog can be found here.

Fine_tuning.ipynb contains all the code necessary for fine-tuning the model.


### Dataset
For this project, we will be using publicly available medical data. This dataset is structured as prompt-completion pairs, where users ask medical questions and receive relevant responses from doctors.


For questions or feedback about the project, don't hesitate to reach out to me on [LinkedIn](https://www.linkedin.com/in/siddhesh-sreedar/).


